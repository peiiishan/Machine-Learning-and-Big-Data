{
  "metadata": {
    "name": "LR and RF models",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Now do all the imports to set up the code we will use later. "
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.classification.LogisticRegressionModel\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.feature.StandardScaler\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.linalg.DenseVector\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions\nimport org.apache.spark.sql.types.BooleanType;\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.ml.classification.MultilayerPerceptronClassifier\nimport scala.collection.JavaConversions._\nimport scala.collection.JavaConverters._\nimport org.apache.spark.storage.StorageLevel\nimport scala.collection.immutable.TreeSet;\nimport scala.collection.mutable.SortedSet;\nimport scala.collection.GenSeq;\n\nimport scala.collection.mutable.ListBuffer;\nimport org.apache.spark.ml.PipelineModel;\nimport org.apache.spark.ml.PipelineStage;\nimport org.apache.spark.ml.Pipeline;\nimport org.apache.spark.ml.feature.OneHotEncoder;\nimport org.apache.spark.ml.feature.StringIndexer;\nimport org.apache.spark.ml.feature.StringIndexerModel;\nimport org.apache.spark.ml.linalg.Vector;\n\nimport org.apache.spark.ml.classification.ProbabilisticClassificationModel;\nimport org.apache.spark.ml.classification.ProbabilisticClassifier;\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## print versions"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// Print versions.\n\nprintln(\"Scala version: \" + util.Properties.versionString);\nprintln(\"Spark version: \" + sc.version);\nprintln(\"Java version: \" + System.getProperty(\"java.version\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Assemble functions to be used later. \n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval b2d \u003d (x:Boolean) \u003d\u003e if(x) 1.0 else 0.0\nval b2dUdf \u003d udf(b2d);\n\ndef filterData (raw : DataFrame) : DataFrame \u003d {\n    var df_filtered \u003d raw.filter($\"MTM_LTV\" \u003e 0.0).filter($\"MTM_LTV\" \u003c 2.0);\n    df_filtered \u003d df_filtered.filter($\"INCENTIVE\" \u003e -1.0).filter($\"CREDIT_SCORE\" \u003e 0);\n    df_filtered \u003d df_filtered.filter($\"NEXT_STATUS\" \u003e\u003d 0).filter($\"NEXT_STATUS\" \u003c\u003d 2);\n    df_filtered \u003d df_filtered.filter($\"AGE\" \u003e\u003d 0).filter($\"STATUS\" \u003d\u003d\u003d 1);\n    df_filtered \u003d df_filtered.filter($\"TERM\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"MI_PERCENT\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"UNIT_COUNT\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"ORIG_LTV\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"ORIG_CLTV\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"ORIG_DTI\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"ORIG_UPB\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"ORIG_INTRATE\" \u003e\u003d 0);\n    \n    // Go ahead and add the status columns here. \n    df_filtered \u003d df_filtered.withColumn(\"actual_p\", expr(\" case when next_status \u003d 0 then 1.0 else 0.0 end\"));\n    df_filtered \u003d df_filtered.withColumn(\"actual_c\", expr(\" case when next_status \u003d 1 then 1.0 else 0.0 end\"));\n    df_filtered \u003d df_filtered.withColumn(\"actual_3\", expr(\" case when next_status \u003d 2 then 1.0 else 0.0 end\"));\n    \n    return df_filtered;\n};\n\ndef convertBoolColumns(data: DataFrame, enumColumns: Array[String]) : DataFrame \u003d {\n    var d2 \u003d data;\n    val type_map \u003d data.dtypes.groupBy(_._1).map { case (k,v) \u003d\u003e (k,v.map(_._2))};\n    \n    for(cname \u003c- enumColumns) {\n        if(type_map(cname)(0).equals(\"BooleanType\")) {\n            // We need to convert this to a string type. \n            d2 \u003d d2.withColumn(cname, b2dUdf(col(cname))); \n        }\n    }\n    \n    return d2;\n}\n\n    "
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n\n// This function generates the appropriate encoders for a dataframe. \ndef generateEncoders(data: DataFrame, enumColumns: Array[String]) : PipelineModel \u003d {\n    val indexCols \u003d enumColumns.map((s:String) \u003d\u003e s + \"_Index\")\n    val vecCols \u003d enumColumns.map((s:String) \u003d\u003e s + \"_Vec\")\n    var stages  \u003d new ListBuffer[PipelineStage]();\n    \n    for(i \u003c- 0 to indexCols.length - 1) {\n        val strCol \u003d enumColumns(i);\n        val indexCol \u003d indexCols(i);\n        val vecCol \u003d vecCols(i);\n\n        \n        var indexer \u003d new StringIndexer().setInputCol(strCol).setOutputCol(indexCol)\n        .setStringOrderType(\"frequencyAsc\");\n        stages.append(indexer);\n    }\n    \n    var onehot \u003d new OneHotEncoder().setInputCols(indexCols).setOutputCols(vecCols).setDropLast(true);\n    stages.append(onehot);\n    \n    val pipeline \u003d new Pipeline()\n        .setStages(stages.toArray)\n    \n    return pipeline.fit(data);\n}\n\n\ndef applyEncoders(data: DataFrame, model: PipelineModel, binaryCols : ListBuffer[String]) : DataFrame \u003d {\n    binaryCols.clear();\n    var df_output \u003d model.transform(data);\n    \n    val vToA: Any \u003d\u003e Array[Double] \u003d _.asInstanceOf[Vector].toArray\n    val vToAUdf \u003d udf(vToA);\n    \n    \n    for(next \u003c- model.stages) {\n        if(next.isInstanceOf[StringIndexerModel]) {\n            // Add a column for each potential value. \n            var indexer \u003d next.asInstanceOf[StringIndexerModel]\n            val labels \u003d indexer.labels;\n            val inputCol \u003d indexer.getInputCol;\n            \n            for(i \u003c- 0 to labels.length - 2) {\n                val labelName \u003d labels(i).replaceAll(\"[^a-zA-Z0-9_]\", \"_\");\n                val colName \u003d (inputCol + \"_\" + labelName);\n                \n                df_output \u003d df_output.withColumn(colName, element_at(vToAUdf(col(inputCol + \"_Vec\")), 1 + i))\n                binaryCols +\u003d colName;\n            }\n        }\n    }\n    \n    return df_output;\n    }\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nimport org.apache.spark.ml.functions.vector_to_array\nimport org.apache.spark.ml.Estimator\n\n// Super ugly. The Probability array is a DenseVector, and basically none of the functions work on it. \ndef convertProbArray (raw : DataFrame, table_name : String) : DataFrame \u003d {\n    return raw.withColumn(\"prob_array\",vector_to_array(functions.col(\"probability\")));\n};\n\ndef expandColumns (raw : DataFrame, table_name : String) : DataFrame \u003d {\n    var expanded \u003d convertProbArray(raw, table_name);\n    \n    val distEntropy \u003d (x: GenSeq[Double]) \u003d\u003e {\n        var sum \u003d 0.0;\n        \n        for(next \u003c- x) {\n            if(next \u003e 0) {\n                sum \u003d sum + (next*Math.log(next))\n            }\n        }\n        \n         -1.0 * sum    \n    }\n    \n    val deUdf \u003d udf(distEntropy)\n    \n    // Now split out the probabilities.\n    expanded \u003d expanded.withColumn(\"prob_p\", element_at($\"prob_array\", 1)).withColumn(\"prob_c\",element_at($\"prob_array\", 2)).withColumn(\"prob_3\", element_at($\"prob_array\", 3));\n    expanded \u003d expanded.withColumn(\"h_g\", deUdf(col(\"prob_array\")));\n    expanded \u003d expanded.withColumn(\"h_fg\", expr(\" -1.0 * log(prob_array[next_status])\"));\n    expanded \u003d expanded.withColumn(\"table_name\", functions.lit(table_name));\n\n    expanded.registerTempTable(table_name);\n    return expanded;\n};\n\ndef testClassifier (df_test : DataFrame, model : PipelineModel, tableName: String) : DataFrame \u003d {\n    return expandColumns(model.transform(df_test),tableName);\n}\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\ndef generateAssembler(data: DataFrame, featureCols: Array[String]) : PipelineModel \u003d {\n    var features \u003d SortedSet[String]()\n    \n    for(next \u003c- featureCols) {\n        if(data.columns.contains(next)) {\n            features +\u003d next;\n        }\n        else if(data.columns.contains(next + \"_Vec\")) {\n            features +\u003d (next + \"_Vec\");\n        }\n        else {\n            throw new IllegalArgumentException(\"Invalid column: \" + next);\n        }\n    }\n\n    val assembler \u003d new VectorAssembler().\n      setInputCols(features.toArray).\n      setOutputCol(\"features\")\n    \n    val scaler \u003d new StandardScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\")\n      .setWithStd(true)\n      .setWithMean(true)\n\n    val pipeline \u003d new Pipeline()\n        .setStages(Array[PipelineStage](assembler, scaler));\n    \n    return pipeline.fit(data); \n}\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval enumCols \u003d SortedSet[String]() ++ Array(\"OCCUPANCY_STATUS\", \"FIRSTTIME_BUYER\", \"TERM\", \"UNIT_COUNT\", \"PREPAYMENT_PENALTY\")\nval numericCols \u003d SortedSet[String]() ++ Array(\"AGE\", \"MTM_LTV\", \"INCENTIVE\", \"CREDIT_SCORE\", \"MI_PERCENT\", \"ORIG_CLTV\", \"ORIG_DTI\", \"ORIG_UPB\", \"ORIG_INTRATE\")\n\nvar df_raw : DataFrame \u003d convertBoolColumns(filterData(spark.read.parquet(\"/Users/peishanliu/Desktop/9733BD/9733twrepo/fregy-9733-w2/data/df_c_1\")), enumCols.toArray)\ndf_raw.registerTempTable(\"df_raw\")\n\nvar binaryCols \u003d new ListBuffer[String]();\nvar pipeline \u003d generateEncoders(df_raw, enumCols.toArray);\ndf_raw \u003d applyEncoders(df_raw, pipeline, binaryCols);\n\nprintln(\"Cols: \" + binaryCols)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql \nselect age, count(1) AS value\nfrom df_raw\ngroup by age\norder by age"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Prepare the data\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// For now, just allow all features. \nval featureList \u003d Array(\"OCCUPANCY_STATUS\", \"FIRSTTIME_BUYER\", \"TERM\", \"UNIT_COUNT\", \"PREPAYMENT_PENALTY\", \"AGE\", \"MTM_LTV\", \"INCENTIVE\", \"CREDIT_SCORE\", \"MI_PERCENT\", \"ORIG_CLTV\", \"ORIG_DTI\", \"ORIG_UPB\", \"ORIG_INTRATE\");\nval fittingSample : Int \u003d 500 * 1000;\n\nval assembler \u003d generateAssembler(df_raw, featureList);\n//val df_assembled \u003d assembler.transform(df_raw);\n\n// Split these, allocating about 500k observations for training. \nvar Array(df_train, df_test) \u003d df_raw.randomSplit(Array(0.25, 0.75), seed\u003d 12345);\n\n// Randomize the order of the training data. \n// This is small enough that we can pretty easily cache the whole thing.\ndf_train \u003d df_train.orderBy(rand()).persist();\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Fit LR and RF Models\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark \n\ndef generate_summary(df_test : DataFrame, model : PipelineModel, tableName: String) : DataFrame \u003d {\n    val results \u003d testClassifier(df_test, model,tableName);\n    val summary \u003d results.groupBy(\"table_name\").agg(\n        functions.count(\"*\").as(\"count\"), functions.avg(\"h_g\").as(\"distEntropy\"), functions.avg(\"h_fg\").as(\"h_fg\"), \n        functions.avg(\"prob_p\").as(\"prob_p\"), functions.avg(\"prob_c\").as(\"prob_c\"), functions.avg(\"prob_3\").as(\"prob_3\"), \n        functions.avg(\"actual_p\").as(\"actual_p\"), functions.avg(\"actual_c\").as(\"actual_c\"), functions.avg(\"actual_3\").as(\"actual_3\")\n        )\n    \n    return summary\n}\n\ndef summarize_assembled(df_train : DataFrame, df_test: DataFrame, model : PipelineModel, tableName: String) : DataFrame \u003d {\n    val train_summary \u003d generate_summary(df_train, model, tableName + \"_train\")\n    val test_summary \u003d generate_summary(df_test, model, tableName + \"_test\")\n    val output \u003d train_summary.union(test_summary).persist();\n    return output\n}\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Experiment 1(Default)\nvar lr_classifier1 \u003d new LogisticRegression().setMaxIter(100).setRegParam(0.001).setElasticNetParam(0.001)\nlr_classifier1 \u003d lr_classifier1.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar lr_pipeline1 \u003d new Pipeline().setStages(Array(assembler, lr_classifier1))\nvar lrModel1 \u003d lr_pipeline1.fit(df_train.limit(fittingSample))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nvar rf_classifier1 \u003d new RandomForestClassifier().setSeed(12345).setNumTrees(12).setMaxDepth(10)\nrf_classifier1 \u003d rf_classifier1.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar rf_pipeline1 \u003d new Pipeline().setStages(Array(assembler, rf_classifier1))\nvar rf_model1 \u003d rf_pipeline1.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// var featureList \u003d Array(\"OCCUPANCY_STATUS\", \"FIRSTTIME_BUYER\", \"TERM\", \"UNIT_COUNT\", \"PREPAYMENT_PENALTY\", \"AGE\", \"MTM_LTV\", \"INCENTIVE\", \"CREDIT_SCORE\", \"MI_PERCENT\", \"ORIG_CLTV\", \"ORIG_DTI\", \"ORIG_UPB\", \"ORIG_INTRATE\");\nvar rf_insample1 \u003d testClassifier(df_train.limit(fittingSample), rf_model1,\"rf_insample1\");\nvar lr_insample1 \u003d testClassifier(df_train.limit(fittingSample), lrModel1,\"lr_insample1\");\nvar rf_indist1 \u003d testClassifier(df_test, rf_model1,\"rf_indist1\");\nvar lr_indist1 \u003d testClassifier(df_test, lrModel1,\"lr_indist1\");\n\n\nvar df_combined1 \u003d summarize_assembled(df_train.limit(fittingSample), df_test, lrModel1, \"base_lr1\")\ndf_combined1 \u003d df_combined1.union(summarize_assembled(df_train.limit(fittingSample), df_test, rf_model1, \"base_rf1\"))\n\n// df_combined \u003d df_combined.persist()\n// df_combined.createOrReplaceTempView(\"df_combined\")\n// z.show(df_combined)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n\n\ndf_combined1 \u003d df_combined1.persist()\ndf_combined1.createOrReplaceTempView(\"df_combined1\")\nz.show(df_combined1)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Experiment2\nvar lr_classifier2 \u003d new LogisticRegression().setMaxIter(100).setRegParam(0.01).setElasticNetParam(0.001)\nlr_classifier2 \u003d lr_classifier2.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar lr_pipeline2 \u003d new Pipeline().setStages(Array(assembler, lr_classifier2))\nvar lrModel2 \u003d lr_pipeline2.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_classifier2 \u003d new RandomForestClassifier().setSeed(12345).setNumTrees(10).setMaxDepth(10)\nrf_classifier2 \u003d rf_classifier2.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar rf_pipeline2 \u003d new Pipeline().setStages(Array(assembler, rf_classifier2))\nvar rf_model2 \u003d rf_pipeline2.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_insample2 \u003d testClassifier(df_train.limit(fittingSample), rf_model2,\"rf_insample2\");\nvar lr_insample2 \u003d testClassifier(df_train.limit(fittingSample), lrModel2,\"lr_insample2\");\nvar rf_indist2 \u003d testClassifier(df_test, rf_model2,\"rf_indist2\");\nvar lr_indist2 \u003d testClassifier(df_test, lrModel2,\"lr_indist2\");\n\n\nvar df_combined2 \u003d summarize_assembled(df_train.limit(fittingSample), df_test, lrModel2, \"base_lr2\")\ndf_combined2 \u003d df_combined2.union(summarize_assembled(df_train.limit(fittingSample), df_test, rf_model2, \"base_rf2\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf_combined2 \u003d df_combined2.persist()\ndf_combined2.createOrReplaceTempView(\"df_combined2\")\nz.show(df_combined2)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Experiment3\nvar lr_classifier3 \u003d new LogisticRegression().setMaxIter(200).setRegParam(0.001).setElasticNetParam(0.001)\nlr_classifier3 \u003d lr_classifier3.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar lr_pipeline3 \u003d new Pipeline().setStages(Array(assembler, lr_classifier3))\nvar lrModel3 \u003d lr_pipeline3.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_classifier3 \u003d new RandomForestClassifier().setSeed(12345).setNumTrees(14).setMaxDepth(10)\nrf_classifier3 \u003d rf_classifier3.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar rf_pipeline3 \u003d new Pipeline().setStages(Array(assembler, rf_classifier3))\nvar rf_model3 \u003d rf_pipeline3.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_insample3 \u003d testClassifier(df_train.limit(fittingSample), rf_model3,\"rf_insample3\");\nvar lr_insample3 \u003d testClassifier(df_train.limit(fittingSample), lrModel3,\"lr_insample3\");\nvar rf_indist3 \u003d testClassifier(df_test, rf_model3,\"rf_indist3\");\nvar lr_indist3 \u003d testClassifier(df_test, lrModel3,\"lr_indist3\");\n\n\nvar df_combined3 \u003d summarize_assembled(df_train.limit(fittingSample), df_test, lrModel3, \"base_lr3\")\ndf_combined3 \u003d df_combined3.union(summarize_assembled(df_train.limit(fittingSample), df_test, rf_model3, \"base_rf3\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf_combined3 \u003d df_combined3.persist()\ndf_combined3.createOrReplaceTempView(\"df_combined3\")\nz.show(df_combined3)"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Experiment4\nvar lr_classifier4 \u003d new LogisticRegression().setMaxIter(200).setRegParam(0.001).setElasticNetParam(0.01)\nlr_classifier4 \u003d lr_classifier4.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar lr_pipeline4 \u003d new Pipeline().setStages(Array(assembler, lr_classifier4))\nvar lrModel4 \u003d lr_pipeline4.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_classifier4 \u003d new RandomForestClassifier().setSeed(12345).setNumTrees(14).setMaxDepth(12)\nrf_classifier4 \u003d rf_classifier4.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar rf_pipeline4 \u003d new Pipeline().setStages(Array(assembler, rf_classifier4))\nvar rf_model4 \u003d rf_pipeline4.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_insample4 \u003d testClassifier(df_train.limit(fittingSample), rf_model4,\"rf_insample4\");\nvar lr_insample4 \u003d testClassifier(df_train.limit(fittingSample), lrModel4,\"lr_insample4\");\nvar rf_indist4 \u003d testClassifier(df_test, rf_model4,\"rf_indist4\");\nvar lr_indist4 \u003d testClassifier(df_test, lrModel4,\"lr_indist4\");\n\n\nvar df_combined4 \u003d summarize_assembled(df_train.limit(fittingSample), df_test, lrModel4, \"base_lr4\")\ndf_combined4 \u003d df_combined4.union(summarize_assembled(df_train.limit(fittingSample), df_test, rf_model4, \"base_rf4\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf_combined4 \u003d df_combined4.persist()\ndf_combined4.createOrReplaceTempView(\"df_combined4\")\nz.show(df_combined4)"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Experiment5\nvar lr_classifier5 \u003d new LogisticRegression().setMaxIter(80).setRegParam(0.001).setElasticNetParam(0.001)\nlr_classifier5 \u003d lr_classifier5.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar lr_pipeline5 \u003d new Pipeline().setStages(Array(assembler, lr_classifier5))\nvar lrModel5 \u003d lr_pipeline5.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_classifier5 \u003d new RandomForestClassifier().setSeed(12345).setNumTrees(14).setMaxDepth(8)\nrf_classifier5 \u003d rf_classifier5.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar rf_pipeline5 \u003d new Pipeline().setStages(Array(assembler, rf_classifier5))\nvar rf_model5 \u003d rf_pipeline5.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_insample5 \u003d testClassifier(df_train.limit(fittingSample), rf_model5,\"rf_insample5\");\nvar lr_insample5 \u003d testClassifier(df_train.limit(fittingSample), lrModel5,\"lr_insample5\");\nvar rf_indist5 \u003d testClassifier(df_test, rf_model5,\"rf_indist5\");\nvar lr_indist5 \u003d testClassifier(df_test, lrModel5,\"lr_indist5\");\n\n\nvar df_combined5 \u003d summarize_assembled(df_train.limit(fittingSample), df_test, lrModel5, \"base_lr5\")\ndf_combined5 \u003d df_combined5.union(summarize_assembled(df_train.limit(fittingSample), df_test, rf_model5, \"base_rf5\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf_combined5 \u003d df_combined5.persist()\ndf_combined5.createOrReplaceTempView(\"df_combined5\")\nz.show(df_combined5)"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Experiment6\nvar lr_classifier6 \u003d new LogisticRegression().setMaxIter(100).setRegParam(0.0001).setElasticNetParam(0.001)\nlr_classifier6 \u003d lr_classifier6.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar lr_pipeline6 \u003d new Pipeline().setStages(Array(assembler, lr_classifier6))\nvar lrModel6 \u003d lr_pipeline6.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_classifier6 \u003d new RandomForestClassifier().setSeed(12345).setNumTrees(16).setMaxDepth(10)\nrf_classifier6 \u003d rf_classifier6.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar rf_pipeline6 \u003d new Pipeline().setStages(Array(assembler, rf_classifier6))\nvar rf_model6 \u003d rf_pipeline6.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_insample6 \u003d testClassifier(df_train.limit(fittingSample), rf_model6,\"rf_insample6\");\nvar lr_insample6 \u003d testClassifier(df_train.limit(fittingSample), lrModel6,\"lr_insample6\");\nvar rf_indist6 \u003d testClassifier(df_test, rf_model6,\"rf_indist6\");\nvar lr_indist6 \u003d testClassifier(df_test, lrModel6,\"lr_indist6\");\n\n\nvar df_combined6 \u003d summarize_assembled(df_train.limit(fittingSample), df_test, lrModel6, \"base_lr6\")\ndf_combined6 \u003d df_combined6.union(summarize_assembled(df_train.limit(fittingSample), df_test, rf_model6, \"base_rf6\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf_combined6 \u003d df_combined6.persist()\ndf_combined6.createOrReplaceTempView(\"df_combined6\")\nz.show(df_combined6)"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Experiment7\nvar lr_classifier7 \u003d new LogisticRegression().setMaxIter(100).setRegParam(0.0001).setElasticNetParam(0.0005)\nlr_classifier7 \u003d lr_classifier7.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar lr_pipeline7 \u003d new Pipeline().setStages(Array(assembler, lr_classifier7))\nvar lrModel7 \u003d lr_pipeline7.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_classifier7 \u003d new RandomForestClassifier().setSeed(12345).setNumTrees(14).setMaxDepth(14)\nrf_classifier7 \u003d rf_classifier7.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar rf_pipeline7 \u003d new Pipeline().setStages(Array(assembler, rf_classifier7))\nvar rf_model7 \u003d rf_pipeline7.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_insample7 \u003d testClassifier(df_train.limit(fittingSample), rf_model7,\"rf_insample7\");\nvar lr_insample7 \u003d testClassifier(df_train.limit(fittingSample), lrModel7,\"lr_insample7\");\nvar rf_indist7 \u003d testClassifier(df_test, rf_model7,\"rf_indist7\");\nvar lr_indist7 \u003d testClassifier(df_test, lrModel7,\"lr_indist7\");\n\n\nvar df_combined7 \u003d summarize_assembled(df_train.limit(fittingSample), df_test, lrModel7, \"base_lr7\")\ndf_combined7 \u003d df_combined7.union(summarize_assembled(df_train.limit(fittingSample), df_test, rf_model7, \"base_rf7\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf_combined7 \u003d df_combined7.persist()\ndf_combined7.createOrReplaceTempView(\"df_combined7\")\nz.show(df_combined7)"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Best Models from Previous Experiments\nvar lr_classifier \u003d new LogisticRegression().setMaxIter(100).setRegParam(0.0001).setElasticNetParam(0.0005)\nlr_classifier \u003d lr_classifier.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar lr_pipeline \u003d new Pipeline().setStages(Array(assembler, lr_classifier))\nvar lrModel \u003d lr_pipeline.fit(df_train.limit(fittingSample))\nlrModel.save(\"BestlrModel_PeishanLiu_RongCao\")"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nvar rf_classifier \u003d new RandomForestClassifier().setSeed(12345).setNumTrees(14).setMaxDepth(12)\nrf_classifier \u003d rf_classifier.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\nvar rf_pipeline \u003d new Pipeline().setStages(Array(assembler, rf_classifier))\nvar rf_model \u003d rf_pipeline.fit(df_train.limit(fittingSample))\nrf_model.save(\"BestrfModel_PeishanLiu_RongCao\")"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar rf_insample \u003d testClassifier(df_train.limit(fittingSample), rf_model,\"rf_insample\");\nvar lr_insample \u003d testClassifier(df_train.limit(fittingSample), lrModel,\"lr_insample\");\nvar rf_indist \u003d testClassifier(df_test, rf_model,\"rf_indist\");\nvar lr_indist \u003d testClassifier(df_test, lrModel,\"lr_indist\");\n\n\nvar df_combined \u003d summarize_assembled(df_train.limit(fittingSample), df_test, lrModel, \"base_lr\")\ndf_combined \u003d df_combined.union(summarize_assembled(df_train.limit(fittingSample), df_test, rf_model, \"base_rf\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf_combined \u003d df_combined.persist()\ndf_combined.createOrReplaceTempView(\"df_combined\")\nz.show(df_combined)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Now apply the models to the various datasets, and analyze the results. \n"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT age, sum(prob_c), sum(prob_p), sum(prob_3), sum(actual_c), sum(actual_p), sum(actual_3), sum(h_g), sum(h_fg)\nFROM rf_insample\nGROUP BY age\nORDER BY age\n"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT age, sum(prob_c), sum(prob_p), sum(prob_3), sum(actual_c), sum(actual_p), sum(actual_3), sum(h_g), sum(h_fg)\nFROM lr_insample\nGROUP BY age\nORDER BY age\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### END\n"
    }
  ]
}