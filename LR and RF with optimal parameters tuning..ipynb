{
  "metadata": {
    "name": "LR and RF with optimal parameters tuning",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## First, add the following artifacts to your interpreter\n\nArtifact: edu.columbia.tjw:item-spark:1.5.1-spark3\nExclude: com., org., io.*\n\nArtifact: edu.columbia.tjw:item:1.5.1\nExclude: "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Now do all the imports to set up the code we will use later. "
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.classification.LogisticRegressionModel\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.feature.StandardScaler\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.linalg.DenseVector\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.functions\nimport org.apache.spark.sql.types.BooleanType;\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.ml.classification.MultilayerPerceptronClassifier\nimport org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel\nimport scala.collection.JavaConversions._\nimport scala.collection.JavaConverters._\nimport org.apache.spark.storage.StorageLevel\nimport scala.collection.immutable.TreeSet;\nimport scala.collection.mutable.SortedSet;\nimport scala.collection.GenSeq;\n\nimport scala.collection.mutable.ListBuffer;\nimport org.apache.spark.ml.PipelineModel;\nimport org.apache.spark.ml.PipelineStage;\nimport org.apache.spark.ml.Pipeline;\nimport org.apache.spark.ml.feature.OneHotEncoder;\nimport org.apache.spark.ml.feature.StringIndexer;\nimport org.apache.spark.ml.feature.StringIndexerModel;\nimport org.apache.spark.ml.linalg.Vector;\n\nimport org.apache.spark.ml.classification.ProbabilisticClassificationModel;\nimport org.apache.spark.ml.classification.ProbabilisticClassifier;\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## print versions"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// Print versions.\n\nprintln(\"Scala version: \" + util.Properties.versionString);\nprintln(\"Spark version: \" + sc.version);\nprintln(\"Java version: \" + System.getProperty(\"java.version\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Assemble functions to be used later. \n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval b2d \u003d (x:Boolean) \u003d\u003e if(x) 1.0 else 0.0\nval b2dUdf \u003d udf(b2d);\n\ndef filterData (raw : DataFrame) : DataFrame \u003d {\n    var df_filtered \u003d raw.filter($\"MTM_LTV\" \u003e 0.0).filter($\"MTM_LTV\" \u003c 2.0);\n    df_filtered \u003d df_filtered.filter($\"INCENTIVE\" \u003e -1.0).filter($\"CREDIT_SCORE\" \u003e 0);\n    df_filtered \u003d df_filtered.filter($\"NEXT_STATUS\" \u003e\u003d 0).filter($\"NEXT_STATUS\" \u003c\u003d 2);\n    df_filtered \u003d df_filtered.filter($\"AGE\" \u003e\u003d 0).filter($\"STATUS\" \u003d\u003d\u003d 1);\n    df_filtered \u003d df_filtered.filter($\"TERM\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"MI_PERCENT\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"UNIT_COUNT\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"ORIG_LTV\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"ORIG_CLTV\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"ORIG_DTI\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"ORIG_UPB\" \u003e\u003d 0);\n    df_filtered \u003d df_filtered.filter($\"ORIG_INTRATE\" \u003e\u003d 0);\n    \n    // Go ahead and add the status columns here. \n    df_filtered \u003d df_filtered.withColumn(\"actual_p\", expr(\" case when next_status \u003d 0 then 1.0 else 0.0 end\"));\n    df_filtered \u003d df_filtered.withColumn(\"actual_c\", expr(\" case when next_status \u003d 1 then 1.0 else 0.0 end\"));\n    df_filtered \u003d df_filtered.withColumn(\"actual_3\", expr(\" case when next_status \u003d 2 then 1.0 else 0.0 end\"));\n    \n    return df_filtered;\n};\n\ndef convertBoolColumns(data: DataFrame, enumColumns: Array[String]) : DataFrame \u003d {\n    var d2 \u003d data;\n    val type_map \u003d data.dtypes.groupBy(_._1).map { case (k,v) \u003d\u003e (k,v.map(_._2))};\n    \n    for(cname \u003c- enumColumns) {\n        if(type_map(cname)(0).equals(\"BooleanType\")) {\n            // We need to convert this to a string type. \n            d2 \u003d d2.withColumn(cname, b2dUdf(col(cname))); \n        }\n    }\n    \n    return d2;\n}\n\n    "
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n\n// This function generates the appropriate encoders for a dataframe. \ndef generateEncoders(data: DataFrame, enumColumns: Array[String]) : PipelineModel \u003d {\n    val indexCols \u003d enumColumns.map((s:String) \u003d\u003e s + \"_Index\")\n    val vecCols \u003d enumColumns.map((s:String) \u003d\u003e s + \"_Vec\")\n    var stages  \u003d new ListBuffer[PipelineStage]();\n    \n    for(i \u003c- 0 to indexCols.length - 1) {\n        val strCol \u003d enumColumns(i);\n        val indexCol \u003d indexCols(i);\n        val vecCol \u003d vecCols(i);\n\n        \n        var indexer \u003d new StringIndexer().setInputCol(strCol).setOutputCol(indexCol)\n        .setStringOrderType(\"frequencyAsc\");\n        stages.append(indexer);\n    }\n    \n    var onehot \u003d new OneHotEncoder().setInputCols(indexCols).setOutputCols(vecCols).setDropLast(true);\n    stages.append(onehot);\n    \n    val pipeline \u003d new Pipeline()\n        .setStages(stages.toArray)\n    \n    return pipeline.fit(data);\n}\n\n\ndef applyEncoders(data: DataFrame, model: PipelineModel, binaryCols : ListBuffer[String]) : DataFrame \u003d {\n    binaryCols.clear();\n    var df_output \u003d model.transform(data);\n    \n    val vToA: Any \u003d\u003e Array[Double] \u003d _.asInstanceOf[Vector].toArray\n    val vToAUdf \u003d udf(vToA);\n    \n    \n    for(next \u003c- model.stages) {\n        if(next.isInstanceOf[StringIndexerModel]) {\n            // Add a column for each potential value. \n            var indexer \u003d next.asInstanceOf[StringIndexerModel]\n            val labels \u003d indexer.labels;\n            val inputCol \u003d indexer.getInputCol;\n            \n            for(i \u003c- 0 to labels.length - 2) {\n                val labelName \u003d labels(i).replaceAll(\"[^a-zA-Z0-9_]\", \"_\");\n                val colName \u003d (inputCol + \"_\" + labelName);\n                \n                df_output \u003d df_output.withColumn(colName, element_at(vToAUdf(col(inputCol + \"_Vec\")), 1 + i))\n                binaryCols +\u003d colName;\n            }\n        }\n    }\n    \n    return df_output;\n    }\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nimport org.apache.spark.ml.functions.vector_to_array\nimport org.apache.spark.ml.Estimator\n\n// Super ugly. The Probability array is a DenseVector, and basically none of the functions work on it. \ndef convertProbArray (raw : DataFrame, table_name : String) : DataFrame \u003d {\n    return raw.withColumn(\"prob_array\",vector_to_array(functions.col(\"probability\")));\n};\n\ndef expandColumns (raw : DataFrame, table_name : String) : DataFrame \u003d {\n    var expanded \u003d convertProbArray(raw, table_name);\n    \n    val distEntropy \u003d (x: GenSeq[Double]) \u003d\u003e {\n        var sum \u003d 0.0;\n        \n        for(next \u003c- x) {\n            if(next \u003e 0) {\n                sum \u003d sum + (next*Math.log(next))\n            }\n        }\n        \n         -1.0 * sum    \n    }\n    \n    val deUdf \u003d udf(distEntropy)\n    \n    // Now split out the probabilities.\n    expanded \u003d expanded.withColumn(\"prob_p\", element_at($\"prob_array\", 1)).withColumn(\"prob_c\",element_at($\"prob_array\", 2)).withColumn(\"prob_3\", element_at($\"prob_array\", 3));\n    expanded \u003d expanded.withColumn(\"h_g\", deUdf(col(\"prob_array\")));\n    expanded \u003d expanded.withColumn(\"h_fg\", expr(\" -1.0 * log(prob_array[next_status])\"));\n    expanded \u003d expanded.withColumn(\"table_name\", functions.lit(table_name));\n\n    expanded.registerTempTable(table_name);\n    return expanded;\n};\n\ndef doFit[A \u003c: ProbabilisticClassifier[Vector,A,B],B \u003c: ProbabilisticClassificationModel[Vector,B]](classifier : ProbabilisticClassifier[Vector,A,B], assembled_train : DataFrame) : B \u003d {\n    var updated \u003d classifier.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\");\n    \n    val start \u003d  System.currentTimeMillis();\n    val model \u003d updated.fit(assembled_train);\n    val elapsed \u003d System.currentTimeMillis() - start;\n    println(\"Fitting Time[\" + classifier.getClass() + \"] (ms): \" + elapsed);\n    return model;\n}\n\n// used to measure performance of eachh pipeline model\ndef doFit2[A \u003c: ProbabilisticClassifier[Vector,A,B],B \u003c: ProbabilisticClassificationModel[Vector,B]](classifier : ProbabilisticClassifier[Vector,A,B], df_train_limit : DataFrame, assembler : PipelineModel) : PipelineModel \u003d {\n    var updated \u003d classifier.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\");\n    \n    var mlp_pipeline \u003d new Pipeline().setStages(Array(assembler, updated));\n\n    val start \u003d  System.currentTimeMillis();\n    val model \u003d mlp_pipeline.fit(df_train_limit);\n    val elapsed \u003d System.currentTimeMillis() - start;\n    println(\"Fitting Time[\" + classifier.getClass() + \"] : \" + elapsed + \" ms.\");\n    return model;\n}\n\ndef testClassifier (df_test : DataFrame, model : PipelineModel, tableName: String) : DataFrame \u003d {\n    return expandColumns(model.transform(df_test),tableName);\n}\n\ndef testClassifier2 (df_test : DataFrame, model : ProbabilisticClassificationModel[Vector,_], tableName: String) : DataFrame \u003d {\n    return expandColumns(model.transform(df_test),tableName);\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\ndef generateAssembler(data: DataFrame, featureCols: Array[String]) : PipelineModel \u003d {\n    var features \u003d SortedSet[String]()\n    \n    for(next \u003c- featureCols) {\n        if(data.columns.contains(next)) {\n            features +\u003d next;\n        }\n        else if(data.columns.contains(next + \"_Vec\")) {\n            features +\u003d (next + \"_Vec\");\n        }\n        else {\n            throw new IllegalArgumentException(\"Invalid column: \" + next);\n        }\n    }\n\n    val assembler \u003d new VectorAssembler().\n      setInputCols(features.toArray).\n      setOutputCol(\"features\")\n    \n    val scaler \u003d new StandardScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\")\n      .setWithStd(true)\n      .setWithMean(true)\n\n    val pipeline \u003d new Pipeline()\n        .setStages(Array[PipelineStage](assembler, scaler));\n    \n    return pipeline.fit(data); \n}\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval enumCols \u003d SortedSet[String]() ++ Array(\"OCCUPANCY_STATUS\", \"FIRSTTIME_BUYER\", \"TERM\", \"UNIT_COUNT\", \"PREPAYMENT_PENALTY\")\nval numericCols \u003d SortedSet[String]() ++ Array(\"AGE\", \"MTM_LTV\", \"INCENTIVE\", \"CREDIT_SCORE\", \"MI_PERCENT\", \"ORIG_CLTV\", \"ORIG_DTI\", \"ORIG_UPB\", \"ORIG_INTRATE\")\n\nvar df_raw : DataFrame \u003d convertBoolColumns(filterData(spark.read.parquet(\"/Applications/zeppelin-0.10.1-bin-netinst/notebook/HW/df_c_1\")), enumCols.toArray)\ndf_raw.registerTempTable(\"df_raw\")\n\nvar binaryCols \u003d new ListBuffer[String]();\nvar pipeline \u003d generateEncoders(df_raw, enumCols.toArray);\ndf_raw \u003d applyEncoders(df_raw, pipeline, binaryCols);\n\nprintln(\"Cols: \" + binaryCols)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql \nselect age, count(1) AS value\nfrom df_raw\ngroup by age\norder by age"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Prepare the data\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// For now, just allow all features. \nval featureList \u003d Array(\"OCCUPANCY_STATUS\", \"FIRSTTIME_BUYER\", \"TERM\", \"UNIT_COUNT\", \"PREPAYMENT_PENALTY\", \"AGE\", \"MTM_LTV\", \"INCENTIVE\", \"CREDIT_SCORE\", \"MI_PERCENT\", \"ORIG_CLTV\", \"ORIG_DTI\", \"ORIG_UPB\", \"ORIG_INTRATE\");\nval fittingSample : Int \u003d 500 * 1000;\n\nval assembler \u003d generateAssembler(df_raw, featureList);\nval df_assembled \u003d assembler.transform(df_raw);\n\n// Split these, allocating about 500k observations for training. \nvar Array(df_train, df_test) \u003d df_raw.randomSplit(Array(0.25, 0.75), seed\u003d12345);\nvar Array(assembled_train, assembled_test) \u003d df_assembled.randomSplit(Array(0.25, 0.75), seed\u003d 12345);\n\n// Randomize the order of the training data. \n// This is small enough that we can pretty easily cache the whole thing.\ndf_train \u003d df_train.orderBy(rand()).persist();\nassembled_train \u003d assembled_train.orderBy(rand()).persist();\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Fit LR and RF Models\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark \n\ndef generate_summary(df_test : DataFrame, model : PipelineModel, tableName: String) : DataFrame \u003d {\n    val results \u003d testClassifier(df_test, model,tableName);\n    val summary \u003d results.groupBy(\"table_name\").agg(\n        functions.count(\"*\").as(\"count\"), functions.avg(\"h_g\").as(\"distEntropy\"), functions.avg(\"h_fg\").as(\"h_fg\"), \n        functions.avg(\"prob_p\").as(\"prob_p\"), functions.avg(\"prob_c\").as(\"prob_c\"), functions.avg(\"prob_3\").as(\"prob_3\"), \n        functions.avg(\"actual_p\").as(\"actual_p\"), functions.avg(\"actual_c\").as(\"actual_c\"), functions.avg(\"actual_3\").as(\"actual_3\")\n        )\n    \n    return summary\n}\n\ndef summarize_assembled(df_train : DataFrame, df_test: DataFrame, model : PipelineModel, tableName: String) : DataFrame \u003d {\n    val train_summary \u003d generate_summary(df_train, model, tableName + \"_train\")\n    val test_summary \u003d generate_summary(df_test, model, tableName + \"_test\")\n    val output \u003d train_summary.union(test_summary).persist();\n    return output\n}\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// var lr_classifier \u003d new LogisticRegression().setMaxIter(100).setRegParam(0.001).setElasticNetParam(0.001).setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\n// var lr_pipeline \u003d new Pipeline().setStages(Array(assembler, lr_classifier))\n// var lrModel \u003d lr_pipeline.fit(df_train.limit(fittingSample))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// var rf_classifier \u003d new RandomForestClassifier().setSeed(12345).setNumTrees(12).setMaxDepth(10)\n// rf_classifier \u003d rf_classifier.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\")\n// var rf_pipeline \u003d new Pipeline().setStages(Array(assembler, rf_classifier))\n// var rf_model \u003d rf_pipeline.fit(df_train.limit(fittingSample))"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// var rf_insample \u003d testClassifier(df_train.limit(fittingSample), rf_model,\"rf_insample\");\n// var lr_insample \u003d testClassifier(df_train.limit(fittingSample), lrModel,\"lr_insample\");\n// var rf_indist \u003d testClassifier(df_test, rf_model,\"rf_indist\");\n// var lr_indist \u003d testClassifier(df_test, lrModel,\"lr_indist\");\n\n\n// var df_combined \u003d summarize_assembled(df_train.limit(fittingSample), df_test, lrModel, \"base_lr\")\n// df_combined \u003d df_combined.union(summarize_assembled(df_train.limit(fittingSample), df_test, rf_model, \"base_rf\"))\n\n// df_combined \u003d df_combined.persist()\n// df_combined.createOrReplaceTempView(\"df_combined\")\n// z.show(df_combined)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// df_combined \u003d df_combined.persist()\n\n// z.show(df_combined)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// used to measure performance of eachh pipeline model\n// def doFit2[A \u003c: ProbabilisticClassifier[Vector,A,B],B \u003c: ProbabilisticClassificationModel[Vector,B]](classifier : ProbabilisticClassifier[Vector,A,B], df_train_limit : DataFrame, assembler : PipelineModel) : PipelineModel \u003d {\n//     var updated \u003d classifier.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\");\n    \n//     var mlp_pipeline \u003d new Pipeline().setStages(Array(assembler, updated));\n\n//     val start \u003d  System.currentTimeMillis();\n//     val model \u003d mlp_pipeline.fit(df_train_limit);\n//     val elapsed \u003d System.currentTimeMillis() - start;\n//     println(\"Fitting Time[\" + classifier.getClass() + \"] : \" + elapsed + \" ms.\");\n//     return model;\n// }\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Default\nval status_count \u003d 3;\nval layers \u003d Array[Int](featureList.size, 8, 6, status_count);\n// var mlp_classifier \u003d new MultilayerPerceptronClassifier().setLayers(layers).setSeed(1234L).setMaxIter(100).setSolver(\"l-bfgs\");\n// mlp_classifier \u003d mlp_classifier.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"scaledFeatures\");\n// var mlp_pipeline \u003d new Pipeline().setStages(Array(assembler, mlp_classifier));\n\n// val start \u003d  System.currentTimeMillis();\n// var mlp_mode1 \u003d mlp_pipeline.fit(df_train.limit(fittingSample));\n// val elapsed \u003d System.currentTimeMillis() - start;\n// println(\"Fitting Time[ mlp_model ] (ms): \" + elapsed);\n   \n   // need to use assembled_train\n// val mlp_model \u003d doFit(new MultilayerPerceptronClassifier().setLayers(layers).setSeed(1234L).setMaxIter(100).setSolver(\"l-bfgs\"), assembled_train.limit(fittingSample));\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// use pipeline model to fit\n// val mlp_model \u003d doFit2(new MultilayerPerceptronClassifier().setLayers(layers).setSeed(1234L).setMaxIter(100).setSolver(\"l-bfgs\"), df_train.limit(fittingSample),assembler);\n\nvar mlp_insample \u003d testClassifier(df_train.limit(fittingSample), mlp_model,\"mlp_insample\");\nvar mlp_indist \u003d testClassifier(df_test, mlp_model,\"mlp_indist\");"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf_combined \u003d df_combined.union(summarize_assembled(df_train.limit(fittingSample), df_test, mlp_model, \"base_mlp\")).persist()\n// z.show(df_combined)"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n(select \"mlp_indist\" AS label,count(*) AS count,  avg(h_g),sum(h_fg), avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_indist ) \nUNION\n(select \"mlp_insample\" AS label,count(*) AS count,  avg(h_g),sum(h_fg), avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_insample )\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval layers2 \u003d Array[Int](featureList.size,8 ,6, status_count)\nval mlp_model2 \u003d doFit2(new MultilayerPerceptronClassifier().setLayers(layers2).setSeed(1234L).setMaxIter(100).setSolver(\"gd\"), df_train.limit(fittingSample),assembler);\n\nmlp_model2.save(\"/Applications/zeppelin-0.10.1-bin-netinst/notebook/HW/savedModel_151p_\" + edu.columbia.tjw.item.util.random.RandomTool.randomString(10) + \".dat\");\n\nvar mlp_insample2 \u003d testClassifier(df_train.limit(fittingSample), mlp_model2,\"mlp_insample_gd\");\nvar mlp_indist2 \u003d testClassifier(df_test, mlp_model2,\"mlp_indist_gd\");\n\n// register this DataFrame first before querying it via %spark.sql\nmlp_insample2.createOrReplaceTempView(\"mlp_insample2\")\nmlp_indist2.createOrReplaceTempView(\"mlp_indist2\")\n// df_combined \u003d df_combined.union(summarize_assembled(df_train.limit(fittingSample), df_test, mlp_model2, \"mlp_gd\")).persist()\n// z.show(df_combined)"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n(select \"mlp_indist2\" AS label,count(*) AS count,  avg(h_g), sum(h_fg),avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_indist2 ) \nUNION\n(select \"mlp_insample2\" AS label,count(*) AS count,  avg(h_g),sum(h_fg), avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_insample2 )\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval layers3 \u003d Array[Int](featureList.size, 4, 4, 6, status_count)\nval mlp_model3 \u003d doFit2(new MultilayerPerceptronClassifier().setLayers(layers3).setSeed(1234L).setMaxIter(100).setSolver(\"l-bfgs\"), df_train.limit(fittingSample),assembler);\nvar mlp_insample3 \u003d testClassifier(df_train.limit(fittingSample), mlp_model3,\"mlp_insample_3\");\nvar mlp_indist3 \u003d testClassifier(df_test, mlp_model3,\"mlp_indist_3\");\n\nmlp_insample3.createOrReplaceTempView(\"mlp_insample3\")\nmlp_indist3.createOrReplaceTempView(\"mlp_indist3\")\n\ndf_combined \u003d df_combined.union(summarize_assembled(df_train.limit(fittingSample), df_test, mlp_model3, \"mlp_3\")).persist()\n// z.show(df_combined)"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n(select \"mlp_indist3\" AS label,count(*) AS count,  avg(h_g),sum(h_fg), avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_indist3 ) \nUNION\n(select \"mlp_insample3\" AS label,count(*) AS count,  avg(h_g),sum(h_fg), avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_insample3 )\n"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval layers4  \u003d Array[Int](featureList.size, 4, 4, 6, status_count)\n// setMaxIter(50\n\nval mlp_model4 \u003d doFit2(new MultilayerPerceptronClassifier().setLayers(layers4).setSeed(1234L).setMaxIter(300).setSolver(\"l-bfgs\"), df_train.limit(fittingSample),assembler);\n\nmlp_model4.save(\"/Applications/zeppelin-0.10.1-bin-netinst/notebook/HW/savedModel_151p_\" + edu.columbia.tjw.item.util.random.RandomTool.randomString(10) + \".dat\");\n\nvar mlp_insample4 \u003d testClassifier(df_train.limit(fittingSample), mlp_model4,\"mlp_insample_4\");\nvar mlp_indist4 \u003d testClassifier(df_test, mlp_model4,\"mlp_indist_4\");\n\nmlp_insample4.createOrReplaceTempView(\"mlp_insample4\")\nmlp_indist4.createOrReplaceTempView(\"mlp_indist4\")\n\n// var df_combined \u003d summarize_assembled(df_train.limit(fittingSample), df_test, mlp_model4, \"mlp_4\")\n// df_combined.persist()\n// df_combined \u003d df_combined.union(summarize_assembled(df_train.limit(fittingSample), df_test, mlp_model4, \"mlp_4\")).persist()\n// z.show(df_combined)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n(select \"mlp_indist4\" AS label,count(*) AS count,  avg(h_g),sum(h_fg), avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_indist4 ) \nUNION\n(select \"mlp_insample4\" AS label,count(*) AS count,  avg(h_g),sum(h_fg), avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_insample4 )\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval layers5 \u003d Array[Int](featureList.size, 4, 4, 3, 3, status_count)\nval mlp_model5 \u003d doFit2(new MultilayerPerceptronClassifier().setLayers(layers5).setSeed(1234L).setMaxIter(200).setSolver(\"l-bfgs\"), df_train.limit(fittingSample),assembler);\nvar mlp_insample5 \u003d testClassifier(df_train.limit(fittingSample), mlp_model5,\"mlp_insample_5\");\nvar mlp_indist5 \u003d testClassifier(df_test, mlp_model5,\"mlp_indist_5\");\nmlp_insample5.createOrReplaceTempView(\"mlp_insample5\")\nmlp_indist5.createOrReplaceTempView(\"mlp_indist5\")\n\ndf_combined \u003d df_combined.union(summarize_assembled(df_train.limit(fittingSample), df_test, mlp_model5, \"mlp_5\")).persist()\n// z.show(df_combined)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// z.show(df_combined)"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n(select \"mlp_indist5\" AS label,count(*) AS count,  avg(h_g),sum(h_fg), avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_indist5 ) \nUNION\n(select \"mlp_insample5\" AS label,count(*) AS count,  avg(h_g),sum(h_fg), avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_insample5 )\n"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval layers6 \u003d Array[Int](featureList.size, 4,2,2,2,4, status_count)\nval mlp_model6 \u003d doFit2(new MultilayerPerceptronClassifier().setLayers(layers6).setSeed(1234L).setMaxIter(200).setSolver(\"l-bfgs\"), df_train.limit(fittingSample),assembler);\nvar mlp_insample6 \u003d testClassifier(df_train.limit(fittingSample), mlp_model6,\"mlp_insample_6\");\nvar mlp_indist6 \u003d testClassifier(df_test, mlp_model6,\"mlp_indist_6\");\nmlp_insample6.createOrReplaceTempView(\"mlp_insample6\")\nmlp_indist6.createOrReplaceTempView(\"mlp_indist6\")\n\n// df_combined \u003d df_combined.union(summarize_assembled(df_train.limit(fittingSample), df_test, mlp_model6, \"mlp_6\")).persist()\n// z.show(df_combined)"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nvar df_combined \u003d summarize_assembled(df_train.limit(fittingSample), df_test, mlp_model6, \"mlp_6\")\ndf_combined \u003d df_combined.persist()\n\n// df_combined \u003d df_combined.union(summarize_assembled(df_train.limit(fittingSample), df_test, mlp_model6, \"mlp_6\")).persist()\nz.show(df_combined)"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n(select \"mlp_indist6\" AS label,count(*) AS count,  avg(h_g),sum(h_fg), avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_indist6 ) \nUNION\n(select \"mlp_insample6\" AS label,count(*) AS count,  avg(h_g),sum(h_fg), avg(h_fg), avg(prob_p), avg(actual_p), avg(prob_c), avg(actual_c), avg(prob_3), avg(actual_3) from mlp_insample6 )\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Now try to fit an ITEM model\n\nMake sure to save your fit, the fitting is quite expensive and you don\u0027t want to be doing this over and over again. \n\nHowever, if you change the features column, you will need to redo the fit, FYI. \n"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// Load the item code. \nimport edu.columbia.tjw.item.spark.ItemClassifier;\nimport edu.columbia.tjw.item.spark.ItemClassificationModel;\nimport edu.columbia.tjw.item.spark.ItemClassifierSettings;\nimport edu.columbia.tjw.item.base.SimpleStatus;\nimport edu.columbia.tjw.item.util.EnumFamily;\nimport edu.columbia.tjw.item.spark.ItemClassifierSettings;\n\ndef generateItemSettings(data : DataFrame, features : Array[String], numericCols : Array[String], paramCount : Integer) : ItemClassifierSettings \u003d {\n    var featureList \u003d features.toList.asJava;\n    \n    var curves \u003d new java.util.TreeSet[String];\n    curves.addAll(featureList);\n    curves.retainAll(numericCols.toList.asJava);\n    \n    var settings \u003d ItemClassifier.prepareSettings(data, \"NEXT_STATUS\", featureList, curves, paramCount);\n    return settings;\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n\nif(false) {\n    // ITEM models are controlled by telling it how many parameters it can add. It might stop before (if it can\u0027t find any more good ones), so this is just an upper limit.\n    // Setting this to a huge value won\u0027t matter beyond a certain point (probably around 30 or so for our dataset sizes)\n    // N.B: This is the number allowed for curves, enum columns will not count against this limit, so the effective parameter count reported will often be higher. \n    var parametersAllowed \u003d 30\n    \n    var settings \u003d generateItemSettings(df_raw, Array.concat((numericCols).toArray, binaryCols.toArray), numericCols.toArray, parametersAllowed);\n    var assembler \u003d ItemClassifier.prepareAssembler(settings, \"features\").setHandleInvalid(\"skip\");\n    var classifier \u003d new ItemClassifier(settings)\n    classifier \u003d classifier.setLabelCol(\"NEXT_STATUS\").setFeaturesCol(\"features\")\n    \n    var item_pipeline \u003d new Pipeline().setStages(Array(assembler, classifier))\n    var itemModel \u003d item_pipeline.fit(df_train.limit(fittingSample).drop(classifier.getFeaturesCol))\n\n    itemModel.save(\"/Users/tyler/sync-workspace/code/spark-notebooks/savedModel_151p_\" + edu.columbia.tjw.item.util.random.RandomTool.randomString(10) + \".dat\");\n}\n\n\n\n\n// var item_pipeline \u003d PipelineModel.load(\"/Users/tyler/sync-workspace/code/spark-notebooks/savedModel_151p_49dfa23b9b.dat\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nvar item_insample \u003d testClassifier(df_train.limit(fittingSample), item_pipeline,\"item_insample\");\nvar item_indist \u003d testClassifier(df_test, item_pipeline,\"item_indist\");"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n\ndf_combined \u003d df_combined.union(summarize_assembled(df_train.limit(fittingSample), df_test, item_pipeline, \"base_item\")).persist()\n\nz.show(df_combined)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Now apply the models to the various datasets, and analyze the results. \n"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT age, sum(prob_c), sum(prob_p), sum(prob_3), sum(actual_c), sum(actual_p), sum(actual_3), sum(h_g), sum(h_fg)\nFROM rf_insample\nGROUP BY age\nORDER BY age\n"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT age, sum(prob_c), sum(prob_p), sum(prob_3), sum(actual_c), sum(actual_p), sum(actual_3), sum(h_g), sum(h_fg)\nFROM lr_insample\nGROUP BY age\nORDER BY age\n"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT age, sum(prob_c), sum(prob_p), sum(prob_3), sum(actual_c), sum(actual_p), sum(actual_3), sum(h_g), sum(h_fg)\nFROM item_insample\nGROUP BY age\nORDER BY age"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT age, sum(prob_c), sum(prob_p), sum(prob_3), sum(actual_c), sum(actual_p), sum(actual_3), sum(h_g), sum(h_fg)\nFROM mlp_insample\nGROUP BY age\nORDER BY age"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### END\n"
    }
  ]
}